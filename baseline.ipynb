{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41d15a9f-0914-4b31-a1ed-5cc9d44cbfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.contrib.handlers.neptune_logger import (\n",
    "    GradsScalarHandler,\n",
    "    NeptuneLogger,\n",
    "    NeptuneSaver,\n",
    "    WeightsScalarHandler,\n",
    "    global_step_from_engine,\n",
    ")\n",
    "from ignite.engine import Events, create_supervised_evaluator, create_supervised_trainer, Engine\n",
    "from ignite.handlers import Checkpoint\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.utils import setup_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b7fcd80-e562-4f17-a945-71a9191ad746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "from helper import F2Metric\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d98aee10-91ed-415c-81be-04e5dc8065a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/home/featurize/data/train_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6efb12b6-4ebe-4d29-aaea-6216c7808b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>video_frame</th>\n",
       "      <th>sequence_frame</th>\n",
       "      <th>image_id</th>\n",
       "      <th>annotations</th>\n",
       "      <th>n_annotations</th>\n",
       "      <th>has_annotations</th>\n",
       "      <th>image_path</th>\n",
       "      <th>subsequence_id</th>\n",
       "      <th>is_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0-0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>video_0/0.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0-1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>video_0/1.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0-2</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>video_0/2.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0-3</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>video_0/3.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0-4</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>video_0/4.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   video_id  sequence  video_frame  sequence_frame image_id annotations  \\\n",
       "0         0     40258            0               0      0-0          []   \n",
       "1         0     40258            1               1      0-1          []   \n",
       "2         0     40258            2               2      0-2          []   \n",
       "3         0     40258            3               3      0-3          []   \n",
       "4         0     40258            4               4      0-4          []   \n",
       "\n",
       "   n_annotations  has_annotations     image_path  subsequence_id  is_train  \n",
       "0              0            False  video_0/0.jpg               1      True  \n",
       "1              0            False  video_0/1.jpg               1      True  \n",
       "2              0            False  video_0/2.jpg               1      True  \n",
       "3              0            False  video_0/3.jpg               1      True  \n",
       "4              0            False  video_0/4.jpg               1      True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/featurize/work/patric/train-validation-split/train-0.1.csv\")\n",
    "df['annotations'] = df['annotations'].apply(eval)\n",
    "df['image_path'] = \"video_\" + df['video_id'].astype(str) + \"/\" + df['video_frame'].astype(str) + \".jpg\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9ca0568-ca71-45b3-ab87-45872b840385",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = df[df['is_train']], df[~df['is_train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13e2cda2-a521-49f1-82a6-7f27aa109202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21576, 1925)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train), len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cddb8573-954d-48c3-b3bd-8d1ad188644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train.annotations.str.len() > 0 ].reset_index(drop=True)\n",
    "df_val = df_val[df_val.annotations.str.len() > 0 ].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a99c8beb-6744-42ab-ae01-1afb5236ec25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4396, 523)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train), len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56941941-b4ae-4ecc-92f0-9abe8655448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReefDataset:\n",
    "\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def can_augment(self, boxes):\n",
    "        \"\"\"Check if bounding boxes are OK to augment\n",
    "\n",
    "        For example: image_id 1-490 has a bounding box that is partially outside of the image\n",
    "        It breaks albumentation\n",
    "        Here we check the margins are within the image to make sure the augmentation can be applied\n",
    "        \"\"\"\n",
    "        \n",
    "        box_outside_image = ((boxes[:, 0] < 0).any() or (boxes[:, 1] < 0).any() \n",
    "                             or (boxes[:, 2] > 1280).any() or (boxes[:, 3] > 720).any())\n",
    "        return not box_outside_image\n",
    "\n",
    "    def get_boxes(self, row):\n",
    "        \"\"\"Returns the bboxes for a given row as a 3D matrix with format [x_min, y_min, x_max, y_max]\"\"\"\n",
    "\n",
    "        boxes = pd.DataFrame(row['annotations'], columns=['x', 'y', 'width', 'height']).astype(float).values\n",
    "\n",
    "        # Change from [x_min, y_min, w, h] to [x_min, y_min, x_max, y_max]\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        return boxes\n",
    "\n",
    "    def get_image(self, row):\n",
    "        \"\"\"Gets the image for a given row\"\"\"\n",
    "\n",
    "        image = cv2.imread(f'{BASE_DIR}/{row[\"image_path\"]}', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        row = self.df.iloc[i]\n",
    "        image = self.get_image(row)\n",
    "        boxes = self.get_boxes(row)\n",
    "        \n",
    "        n_boxes = boxes.shape[0]\n",
    "        \n",
    "        # Calculate the area\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        \n",
    "        target = {\n",
    "            'boxes': torch.as_tensor(boxes, dtype=torch.float32),\n",
    "            'area': torch.as_tensor(area, dtype=torch.float32),\n",
    "            \n",
    "            'image_id': torch.tensor([i]),\n",
    "            \n",
    "            # There is only one class\n",
    "            'labels': torch.ones((n_boxes,), dtype=torch.int64),\n",
    "            \n",
    "            # Suppose all instances are not crowd\n",
    "            'iscrowd': torch.zeros((n_boxes,), dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        if self.transforms and self.can_augment(boxes):\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': target['labels']\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "\n",
    "            if n_boxes > 0:\n",
    "                target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "        else:\n",
    "            image = ToTensorV2(p=1.0)(image=image)['image']\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d82dd47-02df-4769-99e0-cd4dd46c365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Flip(0.5),\n",
    "        # A.Normalize(),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        # A.Normalize(),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab01fde7-48d0-4d00-af7d-70b085d86ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define datasets\n",
    "ds_train = ReefDataset(df_train, get_train_transform())\n",
    "ds_val = ReefDataset(df_val, get_valid_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70f4193b-74b5-4f9e-b948-6b31205b1629",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "lr = 0.0003\n",
    "max_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "233b1249-e767-44e2-9bb9-0085b73150f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
    "dl_val = DataLoader(ds_val, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406a35be-2c93-4429-b515-b164aac960ac",
   "metadata": {},
   "source": [
    "### 模型 & 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad021965-ea4e-421c-b3a4-60f0fe33a6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cool\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\n",
    "model.cuda()\n",
    "optimizer = torch.optim.Adam(lr=0.0003, params=model.parameters())\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "print('cool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ddd9ca4-31c5-4fc9-b31c-74812aa67340",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 12\n",
    "lr = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c05cc3ef-e86c-456f-aaa0-06d8fae91274",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f4b0cd4-fbf4-4f1f-aa14-f709b0c56815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/chenglu.she/patric/e/PAT-21\n"
     ]
    }
   ],
   "source": [
    "npt_logger = NeptuneLogger(\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxMmYwNGVmYS03ODgxLTQ1MzEtOWQ2NS0yMTQwMzBmZGQ0YzcifQ==\",\n",
    "    project_name=\"chenglu.she/patric\",\n",
    "    name=\"baseline\",\n",
    "    params={\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_epochs\": max_epochs,\n",
    "        \"lr\": lr,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"model\": model.__class__.__name__\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5590ace1-c37b-4aa4-aecc-b1e1b4ca1390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(engine, batch):\n",
    "    model.train()\n",
    "    images, targets = batch\n",
    "    images = list(image.to(DEVICE) for image in images)\n",
    "    targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    # Predict\n",
    "    with torch.cuda.amp.autocast(enabled=True):\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    scaler.scale(losses).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    loss_items = {k: v.item() for k, v in loss_dict.items()}\n",
    "    loss_items['sum'] = losses.item()\n",
    "\n",
    "    # loss_classifier, loss_box_reg, loss_objectness, loss_rpn_box_reg, sum\n",
    "    return loss_items\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_loss(engine, batch):\n",
    "    model.train()\n",
    "    images, targets = batch\n",
    "    images = list(image.to(DEVICE) for image in images)\n",
    "    targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    loss_dict = model(images, targets)\n",
    "    losses = sum(loss for loss in loss_dict.values())\n",
    "    loss_dict['sum'] = losses\n",
    "\n",
    "    # loss_classifier, loss_box_reg, loss_objectness, loss_rpn_box_reg, sum\n",
    "    return loss_dict\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_metric(engine, batch):\n",
    "    model.eval()\n",
    "    images, targets = batch\n",
    "    images = list(image.to(DEVICE) for image in images)\n",
    "    targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    output_dict = {\n",
    "        \"model_outputs\": model(images, targets), # [{'boxes': torch.Tensor(N, 4), 'labels': torch.Tensor(N), 'scores': torch.Tensor(N)}, {....}, {....}]\n",
    "        \"images\": images,\n",
    "        \"targets\": targets\n",
    "    }\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a941b4f-59e9-4837-8e71-de19cafd0980",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Engine(update_model)\n",
    "loss_evaluator = Engine(evaluate_loss)\n",
    "evaluator = Engine(evaluate_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9edb500a-8b2a-4d1a-b36c-1335393e57db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ITERATION - loss: 0.00:   0%|          | 0/550 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(initial=0, leave=False, total=len(dl_train), desc=f\"ITERATION - loss: {0:.2f}\")\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED(every=1))\n",
    "def log_training_loss(engine):\n",
    "    pbar.desc = f\"ITERATION - loss: {engine.state.output['sum']:.2f}\"\n",
    "    pbar.update(1)\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED(every=1))\n",
    "def log_training_loss(engine):\n",
    "    pbar.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9a4e85a-95fe-4499-9f65-b99f248c2008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loss_metric(name):\n",
    "    return Loss(\n",
    "        loss_fn=lambda y, _: y,\n",
    "        output_transform=lambda o: (o[name], o[name]),\n",
    "        batch_size=lambda *_: 1,\n",
    "    )\n",
    "\n",
    "metrics = {\n",
    "    \"loss_classifier\": create_loss_metric('loss_classifier'),\n",
    "    \"loss_box_reg\": create_loss_metric('loss_box_reg'),\n",
    "    \"loss_objectness\": create_loss_metric('loss_objectness'),\n",
    "    \"loss_rpn_box_reg\": create_loss_metric('loss_rpn_box_reg'),\n",
    "    \"sum\": create_loss_metric('sum'),\n",
    "}\n",
    "\n",
    "for name, metric in metrics.items():\n",
    "    metric.attach(loss_evaluator, name)\n",
    "\n",
    "score = F2Metric()\n",
    "score.attach(evaluator, 'score')\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def _evaluate_loss(engine):\n",
    "    loss_evaluator.run(dl_val)\n",
    "    evaluator.run(dl_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67ce7826-69ca-4053-babc-677c0bead1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ignite.engine.events.RemovableEventHandle at 0x7f89e9370610>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npt_logger.attach_output_handler(\n",
    "    trainer,\n",
    "    event_name=Events.ITERATION_COMPLETED(every=12),\n",
    "    tag=\"training\",\n",
    "    output_transform=lambda loss: {\"batchloss\": loss[\"sum\"]},\n",
    ")\n",
    "\n",
    "npt_logger.attach_output_handler(\n",
    "    loss_evaluator,\n",
    "    event_name=Events.EPOCH_COMPLETED,\n",
    "    tag=\"val\",\n",
    "    metric_names=[\"loss_classifier\", \"loss_box_reg\", \"loss_objectness\", \"loss_rpn_box_reg\", \"sum\"],\n",
    "    global_step_transform=global_step_from_engine(trainer),\n",
    ")\n",
    "\n",
    "npt_logger.attach_output_handler(\n",
    "    evaluator,\n",
    "    event_name=Events.EPOCH_COMPLETED,\n",
    "    tag=\"val\",\n",
    "    metric_names=[\"score\"],\n",
    "    global_step_transform=global_step_from_engine(trainer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e39bb023-76b0-4138-b007-265de3b62182",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.logger = setup_logger(\"Trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d962db4b-b28d-4318-ab3e-69c2b37d627f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 13:40:07,765 Trainer INFO: Engine run starting with max_epochs=20.\n",
      "/environment/python/versions/miniconda3-4.7.12/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "ITERATION - loss: 0.18: 100%|██████████| 550/550 [05:07<00:00,  2.24it/s]2021-12-08 13:46:36,195 Trainer INFO: Epoch[1] Complete. Time taken: 00:06:28\n",
      "ITERATION - loss: 0.16: 1100it [11:38,  2.19it/s]                        2021-12-08 13:53:03,346 Trainer INFO: Epoch[2] Complete. Time taken: 00:06:27\n",
      "ITERATION - loss: 0.22: 1650it [18:06,  2.24it/s]2021-12-08 13:59:25,657 Trainer INFO: Epoch[3] Complete. Time taken: 00:06:22\n",
      "ITERATION - loss: 0.19: 2200it [24:29,  2.22it/s]2021-12-08 14:05:48,684 Trainer INFO: Epoch[4] Complete. Time taken: 00:06:23\n",
      "ITERATION - loss: 0.23: 2750it [30:52,  2.23it/s]2021-12-08 14:12:12,471 Trainer INFO: Epoch[5] Complete. Time taken: 00:06:24\n",
      "ITERATION - loss: 0.23: 3300it [37:13,  2.28it/s]2021-12-08 14:18:33,956 Trainer INFO: Epoch[6] Complete. Time taken: 00:06:21\n",
      "ITERATION - loss: 0.21: 3850it [43:38,  2.16it/s]2021-12-08 14:24:57,402 Trainer INFO: Epoch[7] Complete. Time taken: 00:06:23\n",
      "ITERATION - loss: 0.21: 4400it [50:01,  2.23it/s]2021-12-08 14:31:22,925 Trainer INFO: Epoch[8] Complete. Time taken: 00:06:26\n",
      "ITERATION - loss: 0.15: 4950it [56:26,  2.17it/s]2021-12-08 14:37:44,535 Trainer INFO: Epoch[9] Complete. Time taken: 00:06:22\n",
      "ITERATION - loss: 0.17: 5500it [1:02:47,  2.22it/s]2021-12-08 14:44:06,132 Trainer INFO: Epoch[10] Complete. Time taken: 00:06:22\n",
      "ITERATION - loss: 0.15: 6050it [1:09:11,  2.22it/s]2021-12-08 14:50:29,521 Trainer INFO: Epoch[11] Complete. Time taken: 00:06:23\n",
      "ITERATION - loss: 0.16: 6600it [1:15:31,  2.17it/s]2021-12-08 14:56:50,162 Trainer INFO: Epoch[12] Complete. Time taken: 00:06:21\n",
      "ITERATION - loss: 0.18: 7150it [1:21:54,  2.17it/s]2021-12-08 15:03:11,279 Trainer INFO: Epoch[13] Complete. Time taken: 00:06:21\n",
      "ITERATION - loss: 0.15: 7700it [1:28:15,  2.22it/s]2021-12-08 15:09:32,504 Trainer INFO: Epoch[14] Complete. Time taken: 00:06:21\n",
      "ITERATION - loss: 0.14: 8250it [1:34:38,  2.13it/s]2021-12-08 15:15:54,795 Trainer INFO: Epoch[15] Complete. Time taken: 00:06:22\n",
      "ITERATION - loss: 0.13: 8800it [1:40:58,  2.23it/s]2021-12-08 15:22:15,034 Trainer INFO: Epoch[16] Complete. Time taken: 00:06:20\n",
      "ITERATION - loss: 0.12: 9350it [1:47:20,  2.09it/s]2021-12-08 15:28:35,680 Trainer INFO: Epoch[17] Complete. Time taken: 00:06:21\n",
      "ITERATION - loss: 0.11: 9900it [1:53:41,  2.16it/s]2021-12-08 15:34:57,586 Trainer INFO: Epoch[18] Complete. Time taken: 00:06:22\n",
      "ITERATION - loss: 0.16: 10450it [2:00:02,  2.23it/s]2021-12-08 15:41:17,624 Trainer INFO: Epoch[19] Complete. Time taken: 00:06:20\n",
      "ITERATION - loss: 0.09: 11000it [2:06:23,  2.20it/s]2021-12-08 15:47:38,259 Trainer INFO: Epoch[20] Complete. Time taken: 00:06:21\n",
      "2021-12-08 15:47:38,263 Trainer INFO: Engine run complete. Time taken: 02:07:30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "State:\n",
       "\titeration: 11000\n",
       "\tepoch: 20\n",
       "\tepoch_length: 550\n",
       "\tmax_epochs: 20\n",
       "\tmax_iters: <class 'NoneType'>\n",
       "\toutput: <class 'dict'>\n",
       "\tbatch: <class 'tuple'>\n",
       "\tmetrics: <class 'dict'>\n",
       "\tdataloader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
       "\tseed: <class 'NoneType'>\n",
       "\ttimes: <class 'dict'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.run(dl_train, max_epochs=max_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

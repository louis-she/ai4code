{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"/home/featurize/work/checkpoint_1_kendall_tau=0.7640.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "state = torch.load(CHECKPOINT_PATH)\n",
    "sys.path.insert(0, f\"/home/featurize/work/ai4code/_runs/{state['params']['git_commits']}\")\n",
    "from ai4code import models, datasets, metrics, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ai4code.metrics' from '/home/featurize/work/ai4code/_runs/e068a5d914827a5195b75dfbb11facb91ee6c4b0/ai4code/metrics.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = state['params']\n",
    "dataset_suffix = params['dataset_suffix']\n",
    "val_folds = params['val_folds']\n",
    "val_num_samples = params['val_num_samples']\n",
    "pretrained_path = params['pretrained_path']\n",
    "extra_vocab = params['extra_vocab']\n",
    "cell_token_size = params['cell_token_size']\n",
    "cell_stride = params['cell_stride']\n",
    "context_cells_token_size = params['context_cells_token_size']\n",
    "context_stride = params['context_stride']\n",
    "max_len = params['max_len']\n",
    "num_workers = params['num_workers']\n",
    "batch_size = params['batch_size']\n",
    "dropout = params['dropout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(f\"/home/featurize/work/ai4code/data/fold0.{params['dataset_suffix']}.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = {k: v for k, v in list(data.items()) if v.fold in val_folds}\n",
    "if val_num_samples is not None:\n",
    "    val_data = {k: v for k, v in list(val_data.items())[:val_num_samples]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_path, do_lower_case=True, use_fast=False\n",
    ")\n",
    "if extra_vocab:\n",
    "    extra_vocab = pickle.load(open(extra_vocab, \"rb\"))\n",
    "    tokenizer.add_tokens(x[0] for x in extra_vocab.most_common(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, ordered_context_ratio):\n",
    "    return datasets.RankDataset(\n",
    "        data,\n",
    "        tokenizer=tokenizer,\n",
    "        cell_token_size=cell_token_size,\n",
    "        cell_stride=cell_stride,\n",
    "        context_cells_token_size=context_cells_token_size,\n",
    "        context_stride=context_stride,\n",
    "        max_len=max_len,\n",
    "        ordered_context_ratio=ordered_context_ratio\n",
    "    )\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    create_dataset(val_data, ordered_context_ratio=0),\n",
    "    num_workers=num_workers,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/featurize/distilbert-base-uncased/distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = models.Model(pretrained_path, dropout)\n",
    "if extra_vocab:\n",
    "    model.backbone.resize_token_embeddings(len(tokenizer))\n",
    "model.to(\"cuda\")\n",
    "model.eval()\n",
    "model.load_state_dict(state['model'])\n",
    "criterion = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def rank_eval(engine, batch):\n",
    "    model.eval()\n",
    "    ids, mask, targets, cell_numbers = [item.to('cuda') for item in batch[:4]]\n",
    "    sample_ids, cell_keys = batch[4:]\n",
    "    scores = model(ids, mask, cell_numbers)\n",
    "    loss = criterion(scores, targets).item()\n",
    "    return loss, scores, sample_ids, cell_keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Engine(rank_eval)\n",
    "ProgressBar().attach(evaluator)\n",
    "metric = metrics.KendallTauNaive(val_data)\n",
    "metric.attach(evaluator, \"kendall_tau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9d073c561f41adad3758c52c82ef48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/126]   1%|           [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kendall Tau:  0.741107666861642\n"
     ]
    }
   ],
   "source": [
    "_ = evaluator.run(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一次运行完毕后，用预测到的 orders 覆盖 val_data 中的 orders 创建一个新的 dataset，use_ordered_context 传入 1\n",
    "# 用新的 dataset 再次运行验证\n",
    "new_val_data = deepcopy(val_data)\n",
    "for sample_id, _ in new_val_data.items():\n",
    "    new_val_data[sample_id].orders = metric._submission_data[sample_id]\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    create_dataset(new_val_data, ordered_context_ratio=1),\n",
    "    num_workers=num_workers,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "new_evaluator = Engine(rank_eval)\n",
    "ProgressBar().attach(new_evaluator)\n",
    "metric_new = metrics.KendallTauNaive(val_data)\n",
    "metric_new.attach(new_evaluator, \"kendall_tau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3949edfc7d44c6da4ef0773846d3fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/126]   1%|           [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kendall Tau:  0.7308870056976112\n"
     ]
    }
   ],
   "source": [
    "_ = new_evaluator.run(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_scores = {}\n",
    "\n",
    "for sample_id, preds in metric._submission_data.items():\n",
    "    sample = val_data[sample_id]\n",
    "    targets = sample.orders\n",
    "    sample_scores[sample_id] = metrics.kendall_tau([targets], [preds])\n",
    "_ = plt.hist(sample_scores.values(), bins=np.arange(0, 1, 0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key:  69820fe579379e  score:  0.42690058479532167\n"
     ]
    }
   ],
   "source": [
    "candinates = {k: v for k, v in sample_scores.items() if v < 0.7}.keys()\n",
    "key_id = random.sample(candinates, k=1)[0]\n",
    "sample = val_data[key_id]\n",
    "print(\"key: \", key_id, \" score: \", sample_scores[key_id])\n",
    "content_targets = \"\"\n",
    "content_preds = \"\"\n",
    "sources = \"\"\n",
    "wrong_sources = \"\"\n",
    "\n",
    "for key in sample.orders:\n",
    "    content_targets += f\"{key}\\t{sample.cell_types[key]}\\n\"\n",
    "\n",
    "    sources += \">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> \" + sample.cell_types[key] + \"\\n\"\n",
    "    sources += sample.sources[key] + \"\\n\\n\\n\"\n",
    "\n",
    "for key in metric._submission_data[sample.id]:\n",
    "    content_preds += f\"{key}\\t{sample.cell_types[key]}\\n\"\n",
    "\n",
    "    wrong_sources += \">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> \" + sample.cell_types[key] + \"\\n\"\n",
    "    wrong_sources += sample.sources[key] + \"\\n\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4520"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"/tmp/1\", \"w\").write(content_targets)\n",
    "open(\"/tmp/2\", \"w\").write(content_preds)\n",
    "open(\"/tmp/source\", \"w\").write(sources)\n",
    "open(\"/tmp/wrong_source\", \"w\").write(wrong_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!diff -y /tmp/1 /tmp/2 > /tmp/3"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b114295533213be714c497b6c7c7c36862ca698da8b4418201631177dea05d47"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
